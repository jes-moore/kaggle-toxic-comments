{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "import pickle\n",
    "import nltk.data\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from files \n",
    "train = pd.read_csv(\"data/train.csv\", header=0)\n",
    "test = pd.read_csv( \"data/test.csv\", header=0)\n",
    "\n",
    "# Verify the number of comments that were read\n",
    "print(\"Read %d labeled train reviews and  %d unlabelled test reviews\" % (len(train),len(test)))\n",
    "\n",
    "all_comments = train['comment_text'].tolist() + test['comment_text'].tolist() \n",
    "\n",
    "with open(\"all_comments.csv\", \"w\") as comments_file:\n",
    "    i=0\n",
    "    for comment in all_comments:\n",
    "        comment = re.sub(\"[^a-zA-Z]\",\" \",str(comment))\n",
    "        comments_file.write(\"%s\\n\" % comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Preprocess Text Reviews For Creating Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>sentences</b> iterable can be simply a list, but for larger corpora, consider a generator that streams the sentences directly from disk/network, without storing everything in RAM. See BrownCorpus, Text8Corpus or LineSentence in the gensim.models.word2vec module for such examples.\n",
    "\n",
    "<b>min_count</b> ignore all words and bigrams with total collected count lower than this.\n",
    "\n",
    "<b>threshold</b> represents a score threshold for forming the phrases (higher means fewer phrases). A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold. see the scoring setting.\n",
    "\n",
    "<b>max_vocab_size</b> is the maximum size of the vocabulary. Used to control pruning of less common words, to keep memory under control. The default of 40M needs about 3.6GB of RAM; increase/decrease max_vocab_size depending on how much available memory you have.\n",
    "\n",
    "<b>delimiter</b> is the glue character used to join collocation tokens, and should be a byte string (e.g. b’_’).\n",
    "\n",
    "<b>scoring</b> specifies how potential phrases are scored for comparison to the threshold setting. scoring can be set with either a string that refers to a built-in scoring function, or with a function with the expected parameter names. Two built-in scoring functions are available by setting scoring to a string:\n",
    "\n",
    "‘default’: from “Efficient Estimaton of Word Representations in Vector Space” by\n",
    "Mikolov, et. al.: (count(worda followed by wordb) - min_count) * N / (count(worda) * count(wordb)) > threshold`, where <b>N</b> is the total vocabulary size.\n",
    "<b>npmi</b>: normalized pointwise mutual information, from “Normalized (Pointwise) Mutual\n",
    "Information in Colocation Extraction” by Gerlof Bouma: ln(prop(worda followed by wordb) / (prop(worda)*prop(wordb))) / - ln(prop(worda followed by wordb) where prop(n) is the count of n / the count of everything in the entire corpus.\n",
    "‘npmi’ is more robust when dealing with common words that form part of common bigrams, and ranges from -1 to 1, but is slower to calculate than the default.\n",
    "\n",
    "To use a custom scoring function, create a function with the following parameters and set the scoring parameter to the custom function. You must use all the parameters in your function call, even if the function does not require all the parameters.\n",
    "\n",
    "<b>worda_count</b>: number of occurrances in sentences of the first token in the phrase being scored wordb_count: number of occurrances in sentences of the second token in the phrase being scored bigram_count: number of occurrances in sentences of the phrase being scored len_vocab: the number of unique tokens in sentences min_count: the min_count setting of the Phrases class corpus_word_count: the total number of (non-unique) tokens in sentences\n",
    "A scoring function without any of these parameters (even if the parameters are not used) will raise a ValueError on initialization of the Phrases class. The scoring function must be picklable.\n",
    "\n",
    "common_terms is an optionnal list of “stop words” that won’t affect frequency count of expressions containing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileToComments(object):    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.stop = set(nltk.corpus.stopwords.words('english'))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        def comment_to_wordlist(comment, remove_stopwords=True):\n",
    "            comment = str(comment)\n",
    "            words = comment.lower().split()\n",
    "            #if remove_stopwords:\n",
    "            #    stops = set(stopwords.words(\"english\"))\n",
    "            #    words = [w for w in words if not w in stops]\n",
    "            return(words)\n",
    "    \n",
    "        for line in open(self.filename, 'r'):\n",
    "            #line = unicode(line, 'utf-8')\n",
    "            tokenized_comment = comment_to_wordlist(line, tokenizer)\n",
    "            yield tokenized_comment\n",
    "        \n",
    "comments = FileToComments('all_comments.csv')\n",
    "#model = gensim.models.Word2Vec(sentences=sentences, window=5, min_count=5, workers=4, hs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bigram = Phrases(comments, min_count=30, threshold=15)\n",
    "bigram_phraser = Phraser(bigram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [bigram_phraser[comment] for comment in comments]\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "clean_tokens = []\n",
    "for token in tokens:\n",
    "    words = [w for w in token if not w in stops]\n",
    "    clean_tokens += [words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pickle the tokens file for further use\n",
    "# import pickle\n",
    "# with open('tokenized_comments.pickle', 'wb') as filename:\n",
    "#     pickle.dump(clean_tokens, filename, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Training and Saving Your Model\n",
    "\n",
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n",
    "\n",
    " - Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results. \n",
    " - Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    " - Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    " - Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300. \n",
    " - Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    " - Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    " - Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-28 08:43:51,274 : INFO : collecting all words and their counts\n",
      "2017-12-28 08:43:51,279 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-28 08:43:51,455 : INFO : PROGRESS: at sentence #10000, processed 332303 words, keeping 35481 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-28 08:43:51,653 : INFO : PROGRESS: at sentence #20000, processed 664751 words, keeping 52404 word types\n",
      "2017-12-28 08:43:51,790 : INFO : PROGRESS: at sentence #30000, processed 998240 words, keeping 66195 word types\n",
      "2017-12-28 08:43:51,922 : INFO : PROGRESS: at sentence #40000, processed 1342842 words, keeping 78285 word types\n",
      "2017-12-28 08:43:52,046 : INFO : PROGRESS: at sentence #50000, processed 1672684 words, keeping 89015 word types\n",
      "2017-12-28 08:43:52,176 : INFO : PROGRESS: at sentence #60000, processed 2007133 words, keeping 98715 word types\n",
      "2017-12-28 08:43:52,299 : INFO : PROGRESS: at sentence #70000, processed 2341001 words, keeping 107515 word types\n",
      "2017-12-28 08:43:52,425 : INFO : PROGRESS: at sentence #80000, processed 2684149 words, keeping 116238 word types\n",
      "2017-12-28 08:43:52,544 : INFO : PROGRESS: at sentence #90000, processed 3020627 words, keeping 124056 word types\n",
      "2017-12-28 08:43:52,673 : INFO : PROGRESS: at sentence #100000, processed 3398527 words, keeping 132573 word types\n",
      "2017-12-28 08:43:52,835 : INFO : PROGRESS: at sentence #110000, processed 3830665 words, keeping 141592 word types\n",
      "2017-12-28 08:43:52,966 : INFO : PROGRESS: at sentence #120000, processed 4230437 words, keeping 150523 word types\n",
      "2017-12-28 08:43:53,111 : INFO : PROGRESS: at sentence #130000, processed 4638940 words, keeping 158220 word types\n",
      "2017-12-28 08:43:53,368 : INFO : PROGRESS: at sentence #140000, processed 5037621 words, keeping 166017 word types\n",
      "2017-12-28 08:43:53,503 : INFO : PROGRESS: at sentence #150000, processed 5430026 words, keeping 173705 word types\n",
      "2017-12-28 08:43:53,679 : INFO : PROGRESS: at sentence #160000, processed 5814654 words, keeping 181318 word types\n",
      "2017-12-28 08:43:53,831 : INFO : PROGRESS: at sentence #170000, processed 6219979 words, keeping 189442 word types\n",
      "2017-12-28 08:43:53,987 : INFO : PROGRESS: at sentence #180000, processed 6628605 words, keeping 196903 word types\n",
      "2017-12-28 08:43:54,141 : INFO : PROGRESS: at sentence #190000, processed 7050797 words, keeping 204479 word types\n",
      "2017-12-28 08:43:54,287 : INFO : PROGRESS: at sentence #200000, processed 7447170 words, keeping 211687 word types\n",
      "2017-12-28 08:43:54,421 : INFO : PROGRESS: at sentence #210000, processed 7841060 words, keeping 218143 word types\n",
      "2017-12-28 08:43:54,565 : INFO : PROGRESS: at sentence #220000, processed 8237093 words, keeping 224674 word types\n",
      "2017-12-28 08:43:54,701 : INFO : PROGRESS: at sentence #230000, processed 8631028 words, keeping 231610 word types\n",
      "2017-12-28 08:43:54,837 : INFO : PROGRESS: at sentence #240000, processed 9038117 words, keeping 238239 word types\n",
      "2017-12-28 08:43:54,969 : INFO : PROGRESS: at sentence #250000, processed 9426690 words, keeping 244940 word types\n",
      "2017-12-28 08:43:55,109 : INFO : PROGRESS: at sentence #260000, processed 9862408 words, keeping 250833 word types\n",
      "2017-12-28 08:43:55,251 : INFO : PROGRESS: at sentence #270000, processed 10259517 words, keeping 257969 word types\n",
      "2017-12-28 08:43:55,405 : INFO : PROGRESS: at sentence #280000, processed 10676016 words, keeping 264403 word types\n",
      "2017-12-28 08:43:55,544 : INFO : PROGRESS: at sentence #290000, processed 11069543 words, keeping 270111 word types\n",
      "2017-12-28 08:43:55,712 : INFO : PROGRESS: at sentence #300000, processed 11477811 words, keeping 276188 word types\n",
      "2017-12-28 08:43:55,927 : INFO : PROGRESS: at sentence #310000, processed 11912148 words, keeping 282692 word types\n",
      "2017-12-28 08:43:56,066 : INFO : PROGRESS: at sentence #320000, processed 12322648 words, keeping 288663 word types\n",
      "2017-12-28 08:43:56,117 : INFO : collected 290436 word types from a corpus of 12441325 raw words and 322849 sentences\n",
      "2017-12-28 08:43:56,118 : INFO : Loading a fresh vocabulary\n",
      "2017-12-28 08:43:57,250 : INFO : min_count=20 retains 28600 unique words (9% of original 290436, drops 261836)\n",
      "2017-12-28 08:43:57,251 : INFO : min_count=20 leaves 11749654 word corpus (94% of original 12441325, drops 691671)\n",
      "2017-12-28 08:43:57,388 : INFO : deleting the raw counts dictionary of 290436 items\n",
      "2017-12-28 08:43:57,397 : INFO : sample=0.001 downsamples 24 most-common words\n",
      "2017-12-28 08:43:57,398 : INFO : downsampling leaves estimated 10648730 word corpus (90.6% of prior 11749654)\n",
      "2017-12-28 08:43:57,399 : INFO : estimated required memory for 28600 words and 300 dimensions: 82940000 bytes\n",
      "2017-12-28 08:43:57,540 : INFO : resetting layer weights\n",
      "2017-12-28 08:43:58,172 : INFO : training model with 4 workers on 28600 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-12-28 08:43:59,193 : INFO : PROGRESS: at 1.06% examples, 510523 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:00,200 : INFO : PROGRESS: at 2.19% examples, 531425 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:01,219 : INFO : PROGRESS: at 3.36% examples, 539393 words/s, in_qsize 7, out_qsize 1\n",
      "2017-12-28 08:44:02,233 : INFO : PROGRESS: at 4.40% examples, 531051 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:03,240 : INFO : PROGRESS: at 5.39% examples, 522955 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:04,250 : INFO : PROGRESS: at 6.15% examples, 500703 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:05,269 : INFO : PROGRESS: at 6.81% examples, 483000 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:06,332 : INFO : PROGRESS: at 7.12% examples, 439427 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:07,340 : INFO : PROGRESS: at 7.95% examples, 441411 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:08,366 : INFO : PROGRESS: at 8.78% examples, 441651 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:09,371 : INFO : PROGRESS: at 9.35% examples, 428799 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:10,401 : INFO : PROGRESS: at 10.03% examples, 421885 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:44:11,417 : INFO : PROGRESS: at 10.80% examples, 421373 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:12,419 : INFO : PROGRESS: at 11.74% examples, 428438 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:13,423 : INFO : PROGRESS: at 12.49% examples, 426804 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:14,431 : INFO : PROGRESS: at 13.09% examples, 420363 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:15,437 : INFO : PROGRESS: at 13.85% examples, 419261 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:44:16,448 : INFO : PROGRESS: at 14.48% examples, 414444 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:17,450 : INFO : PROGRESS: at 15.17% examples, 412387 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:44:18,453 : INFO : PROGRESS: at 16.09% examples, 417302 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:19,458 : INFO : PROGRESS: at 16.82% examples, 415862 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:20,465 : INFO : PROGRESS: at 17.35% examples, 410948 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:44:21,619 : INFO : PROGRESS: at 17.91% examples, 403503 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:22,628 : INFO : PROGRESS: at 18.56% examples, 401367 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:44:23,638 : INFO : PROGRESS: at 19.35% examples, 403554 words/s, in_qsize 8, out_qsize 1\n",
      "2017-12-28 08:44:24,644 : INFO : PROGRESS: at 20.32% examples, 407710 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:25,654 : INFO : PROGRESS: at 21.38% examples, 411424 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:26,667 : INFO : PROGRESS: at 22.48% examples, 416041 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:44:27,675 : INFO : PROGRESS: at 23.62% examples, 420368 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:28,685 : INFO : PROGRESS: at 24.77% examples, 425041 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:29,696 : INFO : PROGRESS: at 25.88% examples, 428807 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:30,757 : INFO : PROGRESS: at 26.41% examples, 424143 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:31,764 : INFO : PROGRESS: at 27.24% examples, 425239 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:32,789 : INFO : PROGRESS: at 28.09% examples, 426401 words/s, in_qsize 8, out_qsize 1\n",
      "2017-12-28 08:44:33,799 : INFO : PROGRESS: at 28.77% examples, 424576 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:34,807 : INFO : PROGRESS: at 29.14% examples, 418336 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-28 08:44:35,817 : INFO : PROGRESS: at 30.07% examples, 419942 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:36,837 : INFO : PROGRESS: at 30.81% examples, 419563 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:37,844 : INFO : PROGRESS: at 31.46% examples, 417734 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:44:38,905 : INFO : PROGRESS: at 32.21% examples, 417144 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:39,906 : INFO : PROGRESS: at 32.82% examples, 415145 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:40,913 : INFO : PROGRESS: at 33.60% examples, 415041 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:41,941 : INFO : PROGRESS: at 34.13% examples, 411798 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:42,956 : INFO : PROGRESS: at 34.82% examples, 410837 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:44:43,959 : INFO : PROGRESS: at 35.52% examples, 409811 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:45,004 : INFO : PROGRESS: at 36.15% examples, 408462 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:46,012 : INFO : PROGRESS: at 36.70% examples, 406055 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:47,019 : INFO : PROGRESS: at 37.38% examples, 405557 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:48,022 : INFO : PROGRESS: at 38.22% examples, 406482 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:49,043 : INFO : PROGRESS: at 39.17% examples, 409057 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:50,063 : INFO : PROGRESS: at 40.15% examples, 411329 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:51,064 : INFO : PROGRESS: at 41.08% examples, 412063 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:52,115 : INFO : PROGRESS: at 41.77% examples, 410205 words/s, in_qsize 8, out_qsize 1\n",
      "2017-12-28 08:44:53,160 : INFO : PROGRESS: at 42.43% examples, 408459 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:54,181 : INFO : PROGRESS: at 43.13% examples, 407093 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:55,188 : INFO : PROGRESS: at 44.15% examples, 408576 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:56,199 : INFO : PROGRESS: at 45.13% examples, 409821 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:44:57,214 : INFO : PROGRESS: at 46.08% examples, 410964 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:58,216 : INFO : PROGRESS: at 46.99% examples, 412763 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:44:59,262 : INFO : PROGRESS: at 47.84% examples, 413374 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:45:00,267 : INFO : PROGRESS: at 48.66% examples, 413884 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:01,279 : INFO : PROGRESS: at 49.32% examples, 412784 words/s, in_qsize 7, out_qsize 1\n",
      "2017-12-28 08:45:02,283 : INFO : PROGRESS: at 50.14% examples, 413057 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:03,289 : INFO : PROGRESS: at 51.10% examples, 414845 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:04,306 : INFO : PROGRESS: at 52.12% examples, 416960 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:05,312 : INFO : PROGRESS: at 53.18% examples, 419268 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:06,312 : INFO : PROGRESS: at 54.18% examples, 420930 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:07,314 : INFO : PROGRESS: at 55.16% examples, 422424 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:08,328 : INFO : PROGRESS: at 55.96% examples, 422551 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:09,352 : INFO : PROGRESS: at 56.79% examples, 422879 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:10,380 : INFO : PROGRESS: at 57.36% examples, 421372 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:45:11,413 : INFO : PROGRESS: at 57.79% examples, 418602 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:12,421 : INFO : PROGRESS: at 58.41% examples, 417597 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:13,448 : INFO : PROGRESS: at 59.10% examples, 417159 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:45:14,507 : INFO : PROGRESS: at 59.68% examples, 415588 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:15,545 : INFO : PROGRESS: at 60.11% examples, 413054 words/s, in_qsize 8, out_qsize 1\n",
      "2017-12-28 08:45:16,577 : INFO : PROGRESS: at 60.79% examples, 411755 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:45:17,603 : INFO : PROGRESS: at 61.40% examples, 410174 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:45:18,655 : INFO : PROGRESS: at 62.04% examples, 408720 words/s, in_qsize 8, out_qsize 1\n",
      "2017-12-28 08:45:19,669 : INFO : PROGRESS: at 62.67% examples, 407491 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:20,712 : INFO : PROGRESS: at 63.27% examples, 405813 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:21,730 : INFO : PROGRESS: at 64.09% examples, 405715 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:22,774 : INFO : PROGRESS: at 64.91% examples, 405488 words/s, in_qsize 8, out_qsize 1\n",
      "2017-12-28 08:45:23,777 : INFO : PROGRESS: at 65.72% examples, 405363 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:24,789 : INFO : PROGRESS: at 66.38% examples, 404869 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:25,796 : INFO : PROGRESS: at 67.01% examples, 404271 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:26,818 : INFO : PROGRESS: at 67.80% examples, 404456 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:27,819 : INFO : PROGRESS: at 68.74% examples, 405742 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:28,827 : INFO : PROGRESS: at 69.67% examples, 406653 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:29,832 : INFO : PROGRESS: at 70.68% examples, 408076 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:45:30,836 : INFO : PROGRESS: at 71.47% examples, 408404 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:31,839 : INFO : PROGRESS: at 72.08% examples, 407720 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:32,848 : INFO : PROGRESS: at 72.74% examples, 407156 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:45:33,875 : INFO : PROGRESS: at 73.26% examples, 405657 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:34,889 : INFO : PROGRESS: at 73.69% examples, 403784 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:35,891 : INFO : PROGRESS: at 74.07% examples, 401715 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:36,916 : INFO : PROGRESS: at 74.70% examples, 401009 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:37,916 : INFO : PROGRESS: at 75.53% examples, 401390 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:38,930 : INFO : PROGRESS: at 76.45% examples, 402436 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:45:39,935 : INFO : PROGRESS: at 77.34% examples, 403384 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:40,938 : INFO : PROGRESS: at 78.30% examples, 404605 words/s, in_qsize 8, out_qsize 1\n",
      "2017-12-28 08:45:41,944 : INFO : PROGRESS: at 79.11% examples, 405135 words/s, in_qsize 7, out_qsize 1\n",
      "2017-12-28 08:45:42,951 : INFO : PROGRESS: at 79.96% examples, 405728 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:45:43,965 : INFO : PROGRESS: at 80.93% examples, 406327 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:44,979 : INFO : PROGRESS: at 82.07% examples, 407694 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:45,992 : INFO : PROGRESS: at 83.12% examples, 408605 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:47,001 : INFO : PROGRESS: at 84.30% examples, 410107 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:48,011 : INFO : PROGRESS: at 85.36% examples, 411165 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:49,016 : INFO : PROGRESS: at 86.38% examples, 412292 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:50,021 : INFO : PROGRESS: at 87.40% examples, 413685 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:45:51,030 : INFO : PROGRESS: at 88.46% examples, 415109 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:45:52,033 : INFO : PROGRESS: at 89.54% examples, 416569 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:53,041 : INFO : PROGRESS: at 90.61% examples, 417892 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:54,046 : INFO : PROGRESS: at 91.57% examples, 418912 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:55,061 : INFO : PROGRESS: at 92.57% examples, 419878 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:45:56,071 : INFO : PROGRESS: at 93.59% examples, 420921 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-28 08:45:57,073 : INFO : PROGRESS: at 94.66% examples, 422235 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:58,082 : INFO : PROGRESS: at 95.75% examples, 423527 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:45:59,095 : INFO : PROGRESS: at 96.80% examples, 424836 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:46:00,100 : INFO : PROGRESS: at 97.82% examples, 425983 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-28 08:46:01,123 : INFO : PROGRESS: at 98.85% examples, 427189 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-28 08:46:02,138 : INFO : PROGRESS: at 99.91% examples, 428434 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-28 08:46:02,185 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-28 08:46:02,206 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-28 08:46:02,214 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-28 08:46:02,216 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-28 08:46:02,217 : INFO : training on 62206625 raw words (53158220 effective words) took 124.0s, 428615 effective words/s\n",
      "2017-12-28 08:46:02,219 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-12-28 08:46:02,517 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-12-28 08:46:02,519 : INFO : not storing attribute syn0norm\n",
      "2017-12-28 08:46:02,523 : INFO : not storing attribute cum_table\n",
      "2017-12-28 08:46:04,213 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "#Load Pre-saved tokenized comments\n",
    "with open('tokenized_comments.pickle', 'rb') as filename:\n",
    "    comments = pickle.load(filename)\n",
    "                           \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 20   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(comments,\n",
    "                          workers=num_workers,\n",
    "                          size=num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window = context,\n",
    "                          sample = downsampling\n",
    "                         )\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Training A Model With Average Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model that we created \n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "#model= gensim.models.KeyedVectors.load_word2vec_format(\"300features_40minwords_10context\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(model.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given comment\n",
    "\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary. \n",
    "    # Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0.:\n",
    "           print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
