{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import time\n",
    "import codecs\n",
    "import functools\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "import urllib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "import pickle\n",
    "import nltk.data\n",
    "import os\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn each comment into a list of word indexes of equal length (with truncation or padding as needed).\n",
    "def comments2Matrix(comment_list,model,maxlen):\n",
    "    \n",
    "    def commentToIndex(comment,index2word_set,model):\n",
    "        indexed_comment = []\n",
    "        # Loop over each word in the comment and, if it is in the model's vocaublary convert it to an index\n",
    "        for word in comment:\n",
    "            if word in index2word_set: \n",
    "                indexed_comment += [model.wv.vocab[word].index]\n",
    "        return [indexed_comment]\n",
    "    \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    totalComments = len(comment_list)\n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=totalComments, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    # Loop over each comment in the comment_list\n",
    "    i=0 #init for progress bar\n",
    "    indexed_comments = [] #init\n",
    "    for comment in comment_list:\n",
    "        indexed_comments += commentToIndex(comment,index2word_set,model)\n",
    "        i += 1\n",
    "        bar.update(i)\n",
    "    bar.finish()\n",
    "    #return indexed_comments\n",
    "    return pad_sequences(indexed_comments,maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15012109662511428730\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from files \n",
    "train = pd.read_csv(\"data/train.csv\", header=0)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 54521 training comments, and 226998 testing comments\n"
     ]
    }
   ],
   "source": [
    "with open('data/tokenized_comments/remove_stops=False.lemmatize=False.spellcheck=True.train_comments.csv') as f:\n",
    "    train_comments = [line.split() for line in f]\n",
    "with open('data/tokenized_comments/remove_stops=False.lemmatize=False.spellcheck=True.test_comments.csv') as f:\n",
    "    test_comments = [line.split() for line in f]\n",
    "print(\"Loaded %s training comments, and %s testing comments\" % (len(train_comments),len(test_comments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Load Pretrained Glove Embeddings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load w2v Model Using Gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "print(\"Loading Gensim Model...\")\n",
    "gensim_file= 'w2v_models/gensim_filtered_models/remove_stops=False.lemmatize=False.spellcheck=True.glove.42B.300d.txt'\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(gensim_file)\n",
    "print(\"Gensim Model Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Convert Comments to a Matrix of Indices</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters for model\n",
    "embed_size = 300 #Embed Size Of Model\n",
    "maxlen = 150 #Max number of words to use for a specific comment\n",
    "max_features = len(word_vectors.wv.vocab) # how many unique words to use (i.e num rows in embedding vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Converting %s comments for training set to matrices' % len(train_comments))\n",
    "xtrain = comments2Matrix(train_comments,word_vectors,maxlen)\n",
    "print('Converting %s comments for testing set to matrices' % len(test_comments))\n",
    "xtest = comments2Matrix(test_comments,word_vectors,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the wv word vectors into a numpy matrix that is suitable for insertion into Keras models\n",
    "embedding_matrix = np.zeros((len(word_vectors.wv.vocab), embed_size))\n",
    "for i in range(len(word_vectors.wv.vocab)):\n",
    "    embedding_vector = word_vectors.wv[word_vectors.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling2D, Reshape,MaxPooling1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv2D, SpatialDropout1D, BatchNormalization, GlobalMaxPooling2D,Conv1D\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, Sequential\n",
    "\n",
    "file_path = \"BD-LSTM-noatt-maxlen100.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1,save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False,name = 'Word-Embedding-Layer')) \n",
    "model.add(Dropout(0.4,name = 'Dropout-Regularization-1')) # Best = 0.3\n",
    "model.add(Bidirectional(LSTM(300, return_sequences=True, dropout=0.25, recurrent_dropout=0.25,kernel_initializer=glorot_normal(seed=None)),name = 'BDLSTM')) #Best = 300,0.25,0.25\n",
    "model.add(GlobalMaxPool1D(name = 'Global-Max-Pool-1d')) \n",
    "model.add(Dense(256, activation=\"relu\",name = 'FC-256')) # Best = 256\n",
    "model.add(Dense(6, activation=\"sigmoid\",name = 'FC-Output-Layer'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(xtrain, y, batch_size=50, epochs=100,validation_split=0.1, callbacks=[checkpoint,early_stop],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling2D, Reshape,MaxPooling1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv2D, SpatialDropout1D, BatchNormalization, GlobalMaxPooling2D,Conv1D\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, Sequential\n",
    "\n",
    "file_path = \"BD-LSTM-noatt.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1,save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False,name = 'Word-Embedding-Layer')) \n",
    "model.add(Dropout(0.3,name = 'Dropout-Regularization-1'))\n",
    "model.add(Bidirectional(LSTM(300, return_sequences=True, dropout=0.25, recurrent_dropout=0.25,kernel_initializer=glorot_normal(seed=None)),name = 'BDLSTM'))\n",
    "model.add(GlobalMaxPool1D(name = 'Global-Max-Pool-1d'))\n",
    "model.add(Dense(256, activation=\"relu\",name = 'FC-256'))\n",
    "model.add(Dense(6, activation=\"sigmoid\",name = 'FC-Output-Layer'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "#history = model.fit(xtrain, y, batch_size=256, epochs=100,validation_split=0.1, callbacks=[checkpoint,early_stop],verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B> MOVE TO ANOTHER NOTEBOOK TO AVOID CONFUSION AND LOST DATA </B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# Good Success with lowering the length - probably erases a lot of padding that confuses the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"BD-LSTM-noatt-maxlen100.hdf5\"\n",
    "model.load_weights(file_path)\n",
    "y_test = model.predict([xtest], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('submissions/sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submissions/no_stops_test_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"BD-LSTM-noatt.hdf5\"\n",
    "model.load_weights(file_path)\n",
    "y_test = model.predict([xtest], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('submissions/sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submissions/glove_vectors_unlemmatized_len50_LSTM.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b> Ensembling Models </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_100 = 'submissions/glove_vectors_unlemmatized_len100_LSTM0043.csv'\n",
    "lstm_50 = 'submissions/glove_vectors_unlemmatized_len50_LSTM.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lstm100 = pd.read_csv(lstm_100)\n",
    "p_lstm50 = pd.read_csv(lstm_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "p_res = p_lstm100.copy()\n",
    "p_res[label_cols] = (p_lstm100[label_cols] + p_lstm50[label_cols]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_res.to_csv('submissions/ensemble_100_50_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
